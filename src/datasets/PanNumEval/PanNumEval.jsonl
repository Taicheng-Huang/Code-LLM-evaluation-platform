{"task_id": "PanNumEval/0", "prompt": "import pandas as pd\nimport numpy as np\n\ndef transferNaN(df):\n    #give a dataframe, replace Numpy NaN with None\n", "entry_point": "transferNaN", "canonical_solution": ["\n    return  df.where(pd.notnull(df), None)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    data = {'A': [1, np.nan, 3],'B': [np.nan, 5, np.nan]} \n    df1 = pd.DataFrame(data)\n    result1 = candidate(df1) \n    expected_result1 = df1.where(pd.notnull(df1), None) \n    assert result1.equals(expected_result1) \n    # Test Case 2: DataFrame without NaN values\n    data2 = {'A': [1, 2, 3],'B': [4, 5, 6]}\n    df2 = pd.DataFrame(data2)\n    result2 = candidate(df2)\n    expected_result2 = df2.where(pd.notnull(df2), None)\n    assert result2.equals(expected_result2) "}
{"task_id": "PanNumEval/1", "prompt": "import pandas as pd\nimport numpy as np\n\ndef createZerosDF():\n    # Create a NumPy array of shape (3, 4) filled with zeros and then convert it into a pandas DataFrame. return the first 2 rows of the DataFrame using the head() function.\n", "entry_point": "createZerosDF", "canonical_solution": ["\n    # Create a NumPy array filled with zeros\n    numpy_array = np.zeros((3, 4))\n    # Convert the NumPy array into a pandas DataFrame\n    df = pd.DataFrame(numpy_array)\n    # Display the first 2 rows of the DataFrame using head()\n    return df.head(2)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    result1 = candidate()\n    expected_result1 = pd.DataFrame(np.zeros((3, 4))).head(2)\n    assert result1.equals(expected_result1)"}
{"task_id": "PanNumEval/2", "prompt": "import pandas as pd\nimport numpy as np\n\n\ndef filterSalesData(df, threshold_sales, year_start, year_end):\n    '''Given a DataFrame sales_data df with columns 'Month', 'Year', and 'Sales', you need to filter out the rows where 'Sales' is less than a threshold_sales  value and the 'Year' is within year_start and year_end (includeborder). Return the modified DataFrame.''' \n", "entry_point": "filterSalesData", "canonical_solution": ["\n    filtered_sales_data = df.loc[\n        (df['Sales'] >= threshold_sales) & (df['Year'].isin(np.arange(year_start, year_end + 1)))]\n    return filtered_sales_data"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'Month': [1, 2, 3, 4, 5, 6],\n        'Year': [2020, 2021, 2021, 2022, 2022, 2023],\n        'Sales': [100, 150, 80, 200, 300, 50]\n    })\n\n    filtered_data = candidate(sample_data, threshold_sales=120, year_start=2021, year_end=2022)\n    expected_result_1 = pd.DataFrame({\n        'Month': [2, 4, 5],\n        'Year': [2021, 2022, 2022],\n        'Sales': [150, 200, 300]\n    })\n    assert np.allclose(expected_result_1.values, filtered_data.values)\n\n    # Test case 2: Edge case with empty DataFrame\n    empty_data = pd.DataFrame(columns=['Month', 'Year', 'Sales'])\n    filtered_empty_data = candidate(empty_data, threshold_sales=100, year_start=2020, year_end=2022)\n    expected_result_2 = pd.DataFrame(columns=['Month', 'Year', 'Sales'])\n    assert filtered_empty_data.equals(expected_result_2)\n\n    # Test case 3: Filtering out all rows\n    sample_data_2 = pd.DataFrame({\n        'Month': [1, 2, 3, 4, 5, 6],\n        'Year': [2020, 2021, 2021, 2022, 2022, 2023],\n        'Sales': [50, 80, 60, 70, 90, 40]\n    })\n    filtered_data_2 = candidate(sample_data_2, threshold_sales=100, year_start=2021, year_end=2022)\n    assert filtered_data_2.empty"}
{"task_id": "PanNumEval/3", "prompt": "import pandas as pd\nimport numpy as np\n\ndef concatenate_and_remove_duplicates(array1, array2, df):\n    #You are given two numpy arrays and a pandas DataFrame. Your task is to write a function to concatenate the arrays and create a new DataFrame with given DataFrame's column while removing any duplicate rows.", "entry_point": "concatenate_and_remove_duplicates", "canonical_solution": ["\n    concatenated_array = np.concatenate((array1, array2), axis=0)\n    concatenated_df = pd.DataFrame(concatenated_array, columns=df.columns)\n    deduplicated_df = concatenated_df.drop_duplicates()\n    return deduplicated_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1\n    array1 = np.array([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9]])\n    array2 = np.array([[4, 5, 6],\n                       [7, 8, 9],\n                       [10, 11, 12]])\n    data = {'A': [4, 5, 6, 7],\n            'B': [8, 9, 10, 11],\n            'C': [12, 13, 14, 15]}\n    df = pd.DataFrame(data, dtype='int32')\n    result_df = candidate(array1, array2, df)\n    expected_result = {'A': [1, 4, 7, 10],\n                       'B': [2, 5, 8, 11],\n                       'C': [3, 6, 9, 12]}\n    expected_df = pd.DataFrame(expected_result, index=[0, 1, 2, 5], dtype='int32')\n    assert result_df.equals(expected_df)\n\n    # Test Case 2\n    array1 = np.array([[1, 2, 3], [4, 5, 6]])\n    array2 = np.array([[7, 8, 9]])\n    columns = ['A', 'B', 'C']\n    df = pd.DataFrame(columns=columns, dtype='int32')\n\n    result = candidate(array1, array2, df)\n    expected_result = pd.DataFrame(np.concatenate((array1, array2), axis=0, dtype='int32'), columns=columns)\n    assert result.equals(expected_result)"}
{"task_id": "PanNumEval/4", "prompt": "import pandas as pd\nimport numpy as np\n\ndef meanAndConcat(df1, df2, columns):\n    '''You are given two pandas DataFrames, df1 and df2, containing numerical data. Your task is to calculate the mean of each column in both DataFrames and then concatenate the resulting means into a new DataFrame with the given columns.'''", "entry_point": "meanAndConcat", "canonical_solution": ["\n    # Calculate mean for each column in both DataFrames\n    mean_df1 = np.mean(df1)\n    mean_df2 = np.mean(df2)\n\n    # Concatenate the mean values into a new DataFrame\n    mean_concatenated = pd.concat([mean_df1, mean_df2], axis=1)\n    mean_concatenated.columns = columns\n    return mean_concatenated"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Example 1\n    df1_example1 = pd.DataFrame({\n        'A': [1, 2, 3],\n        'B': [4, 5, 6]\n    })\n\n    df2_example1 = pd.DataFrame({\n        'A': [7, 8, 9],\n        'B': [10, 11, 12]\n    })\n\n    columns_example1 = ['Mean_A', 'Mean_B']\n\n    result_example1 = candidate(df1_example1, df2_example1, columns_example1)\n    expected_result_example1 = pd.DataFrame({\n        'Mean_A': [4.333333, 8.000000],\n        'Mean_B': [7.666667, 11.000000]\n    })\n\n    assert np.allclose(result_example1, expected_result_example1, rtol=1e-6)\n\n    # Example 2\n    df1_example2 = pd.DataFrame({\n        'X': [10, 20, 30],\n        'Y': [40, 50, 60]\n    })\n\n    df2_example2 = pd.DataFrame({\n        'X': [70, 80, 90],\n        'Y': [100, 110, 120]\n    })\n\n    columns_example2 = ['Avg_X', 'Avg_Y']\n\n    result_example2 = candidate(df1_example2, df2_example2, columns_example2)\n    expected_result_example2 = pd.DataFrame({\n        'Avg_X': [40.0, 80.0],\n        'Avg_Y': [80.0, 110.0]\n    })\n\n    assert np.allclose(result_example2, expected_result_example2, rtol=1e-6)"}
{"task_id": "PanNumEval/5", "prompt": "import pandas as pd\nimport numpy as np\n\ndef matrix_multiplication_and_merge(matrix1, matrix2, key_column, df):\n    ''' You are given two matrices as numpy arrays, a pandas DataFrame, and a key_column name. Perform matrix multiplication using np.dot() on the matrices. Then, merge the resulting matrix with the DataFrame on the specified key_column and return the merged DataFrame.'''", "entry_point": "matrix_multiplication_and_merge", "canonical_solution": ["\n    result_matrix = np.dot(matrix1, matrix2)\n    result_df = pd.DataFrame(result_matrix, columns=df.columns)\n    merged_df = pd.merge(df, result_df, on=key_column)\n    return merged_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Example 1\n    array1 = np.array([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9]])\n    array2 = np.array([[4, 5, 6],\n                       [0, 8, 9],\n                       [0, 11, 12]])\n    data = {'Key': [4, 5, 6, 7],\n            'B': [8, 9, 10, 11],\n            'C': [12, 13, 14, 15]}\n    df = pd.DataFrame(data, dtype='int32')\n    result_df = candidate(array1, array2, 'Key', df)\n    expected_result = {'Key': [4],\n                       'B_x': [8],\n                       'C_x': [12],\n                       'B_y': [54],\n                       'C_y': [60]}\n    expected_df = pd.DataFrame(expected_result, dtype='int32')\n    assert result_df.equals(expected_df)\n\n    # Example 2\n    array1 = np.array([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9]])\n    array2 = np.array([[4, 5, 6],\n                       [0, 8, 9],\n                       [1, 11, 12]])\n    data = {'Key': [4, 22, 6, 37],\n            'B': [8, 9, 10, 11],\n            'C': [12, 13, 14, 15]}\n    df = pd.DataFrame(data, dtype='int32')\n    result_df = candidate(array1, array2, 'Key', df)\n    expected_result = {'Key': [22, 37],\n                       'B_x': [9, 11],\n                       'C_x': [13, 15],\n                       'B_y': [126, 198],\n                       'C_y': [141, 222]}\n    expected_df = pd.DataFrame(expected_result, dtype='int32')\n    assert result_df.equals(expected_df)"}
{"task_id": "PanNumEval/6", "prompt": "import pandas as pd\nimport numpy as np\n\ndef calculate_column_sums_and_rename(df):\n    ''' Given a DataFrame df, calculate the sum of each column using np.sum() and then rename the columns to indicate that they are sum values.'''", "entry_point": "calculate_column_sums_and_rename", "canonical_solution": ["\n    column_sums = np.sum(df, axis=0)\n    column_names = df.columns\n    sum_column_names = [f'{col}_sum' for col in column_names]\n    \n    df_sum = pd.DataFrame([column_sums], columns=sum_column_names)\n    return df_sum"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    data = {'A': [1, 2, 3], 'B': [4, 5, 6]}\n    df = pd.DataFrame(data)\n    result = candidate(df)\n    expected_result = pd.DataFrame([[6, 15]], columns=['A_sum', 'B_sum'])\n    assert result.equals(expected_result)"}
{"task_id": "PanNumEval/7", "prompt": "import pandas as pd\nimport numpy as np\n\ndef calculate_row_sums_and_rename(df):\n    ''' Given a DataFrame df with numerical columns, calculate the sum of each row and add a new column 'Total' with the row sums. Rename the columns by adding a prefix 'Column_' to each column name. Return the modified DataFrame. '''\n", "entry_point": "calculate_row_sums_and_rename", "canonical_solution": ["\n    row_sums = df.apply(np.sum, axis=1)\n    df['Total'] = row_sums\n\n    renamed_df = df.rename(columns=lambda col: 'Column_' + col)\n    return renamed_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    data = {\n        'A': [1, 2, 3],\n        'B': [4, 5, 6],\n        'C': [7, 8, 9]\n    }\n    df = pd.DataFrame(data)\n    \n    result_df = candidate(df)\n    \n    expected_data = {\n        'Column_A': [1, 2, 3],\n        'Column_B': [4, 5, 6],\n        'Column_C': [7, 8, 9],\n        'Total': [12, 15, 18]\n    }\n    expected_df = pd.DataFrame(expected_data)\n    \n    assert result_df.equals(expected_df)\n    \n    # Test Case 2: Edge case with an empty DataFrame\n    empty_df = pd.DataFrame(columns=['A', 'B', 'C'])\n    \n    result_empty_df = candidate(empty_df)\n    \n    expected_empty_df = pd.DataFrame(columns=['Column_A', 'Column_B', 'Column_C', 'Total'])\n    \n    assert result_empty_df.equals(expected_empty_df)\n\n    # Test Case 3: DataFrame with negative values\n    data2 = {\n        'X': [-1, -2, -3],\n        'Y': [-4, -5, -6],\n        'Z': [-7, -8, -9]\n    }\n    df2 = pd.DataFrame(data2)\n    \n    result_df2 = candidate(df2)\n    \n    expected_data2 = {\n        'Column_X': [-1, -2, -3],\n        'Column_Y': [-4, -5, -6],\n        'Column_Z': [-7, -8, -9],\n        'Total': [-12, -15, -18]\n    }\n    expected_df2 = pd.DataFrame(expected_data2)\n    \n    assert result_df2.equals(expected_df2)"}
{"task_id": "PanNumEval/8", "prompt": "import pandas as pd\nimport numpy as np\n\ndef split_and_dropna(df):\n    ''' Given a DataFrame df, split it into two DataFrames based on the presence of NaN values in the 'Value' column. Return the two split DataFrames with NaN rows removed. '''\n", "entry_point": "split_and_dropna", "canonical_solution": ["\n    split_dfs = np.split(df, [df.index[df['Value'].isna()].tolist()[0]])\n    cleaned_dfs = [split.dropna() for split in split_dfs]\n    return cleaned_dfs[0], cleaned_dfs[1]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    data = {'Value': [1.0, 2.0, np.nan, 4.0, np.nan, 6.0]}\n    df = pd.DataFrame(data)\n    result_df1, result_df2 = candidate(df)\n    expected_result_df1 = pd.DataFrame({'Value': [1.0, 2.0]})\n    expected_result_df2 = pd.DataFrame({'Value': [4.0, 6.0]})\n    assert result_df1.equals(expected_result_df1)\n    assert result_df2.equals(expected_result_df2)\n\n    # Test Case 2: DataFrame without NaN values\n    data2 = {'Value': [1.0, 2.0, 3.0, 4.0]}\n    df2 = pd.DataFrame(data2)\n    result_df3, result_df4 = candidate(df2)\n    expected_result_df3 = pd.DataFrame({'Value': [1.0, 2.0, 3.0, 4.0]})\n    expected_result_df4 = pd.DataFrame(columns=['Value'])\n    assert result_df3.equals(expected_result_df3)\n    assert result_df4.equals(expected_result_df4)\n\n    # Test Case 3: Empty DataFrame\n    empty_df = pd.DataFrame(columns=['Value'])\n    result_df5, result_df6 = candidate(empty_df)\n    expected_result_df5 = pd.DataFrame(columns=['Value'])\n    expected_result_df6 = pd.DataFrame(columns=['Value'])\n    assert result_df5.equals(expected_result_df5)\n    assert result_df6.equals(expected_result_df6)\n}"}
{"task_id": "PanNumEval/9", "prompt": "import pandas as pd\nimport numpy as np\n\ndef tile_fill_columns(df, tile_rows, fill_value):\n    ''' Given a DataFrame df, tile its rows with dimensions tile_rows * df.shape[1] using np.tile() and then fill any NaN values in the tiled DataFrame with fill_value. Return the modified DataFrame. '''\n", "entry_point": "tile_fill_columns", "canonical_solution": ["\n    tiled_rows = np.tile(df.values, (tile_rows, 1))\n    tiled_df = pd.DataFrame(tiled_rows, columns=df.columns)\n    filled_df = tiled_df.fillna(fill_value)\n    return filled_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'A': [1, 2, np.nan],\n        'B': [4, np.nan, 6]\n    }, dtype='int32')\n\n    result = candidate(sample_data, tile_rows=3, fill_value=0)\n    expected_result = pd.DataFrame({\n        'A': [1, 2, 0, 1, 2, 0, 1, 2, 0],\n        'B': [4, 0, 6, 4, 0, 6, 4, 0, 6]\n    })\n    assert result.equals(expected_result)\n\n    # Test Case 2: Tiling without NaN values\n    data2 = {'A': [1, 2, 3], 'B': [4, 5, 6]}\n    df2 = pd.DataFrame(data2)\n\n    result2 = candidate(df2, tile_rows=2, fill_value=-1)\n    expected_result2 = pd.DataFrame({\n        'A': [1, 2, 3, 1, 2, 3],\n        'B': [4, 5, 6, 4, 5, 6]\n    })\n    print(result2)\n    assert result2.equals(expected_result2)\n\n    # Test Case 3: Tiling an empty DataFrame\n    empty_data = pd.DataFrame(columns=['A', 'B'])\n\n    result3 = candidate(empty_data, tile_rows=3, fill_value=99)\n    expected_result3 = pd.DataFrame(columns=['A', 'B'])\n    assert result3.empty\n\n    print(","All test cases passed!":")"}
{"task_id": "PanNumEval/10", "prompt": "import pandas as pd\nimport numpy as np\n\ndef repeat_and_sort(df, column_name, repeat_times):\n    ''' Given a DataFrame df, repeat each value in the specified column repeat_times number of times, and then sort the DataFrame based on the values in that column. Return the modified DataFrame. '''\n", "entry_point": "repeat_and_sort", "canonical_solution": ["\n    repeated_values = np.repeat(df[column_name], repeat_times)\n    repeated_df = df.loc[repeated_values.index].copy()\n    repeated_df[column_name] = repeated_values\n    sorted_df = repeated_df.sort_values(by=column_name)\n    return sorted_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'A': [3, 1, 2],\n        'B': [6, 4, 5]\n    })\n\n    result_df = candidate(sample_data, column_name='A', repeat_times=2)\n    expected_result_1 = pd.DataFrame({\n        'A': [1, 1, 2, 2, 3, 3],\n        'B': [4, 4, 5, 5, 6, 6]\n    }, index=[1, 1, 2, 2, 0, 0])\n    assert result_df.equals(expected_result_1)\n\n    # Test Case 2: Edge case with empty DataFrame\n    empty_data = pd.DataFrame(columns=['A', 'B'])\n    result_empty_data = candidate(empty_data, column_name='A', repeat_times=3)\n    expected_result_2 = pd.DataFrame(columns=['A', 'B'])\n    assert result_empty_data.equals(expected_result_2)\n\n    # Test Case 3: Sorting in descending order\n    sample_data_2 = pd.DataFrame({\n        'A': [2, 3, 1],\n        'B': [5, 6, 4]\n    })\n    result_df_2 = candidate(sample_data_2, column_name='A', repeat_times=1)\n    expected_result_3 = pd.DataFrame({\n        'A': [1, 2, 3],\n        'B': [4, 5, 6]\n    }, index=[2, 0, 1])\n    assert result_df_2.equals(expected_result_3)"}
{"task_id": "PanNumEval/11", "prompt": "import pandas as pd\nimport numpy as np\n\ndef transpose_and_stack(array, df):\n    ''' Given a 2D NumPy array and a DataFrame df, transpose the array and then stack it on top of the DataFrame using the column names from the DataFrame. Return the stacked DataFrame. '''\n", "entry_point": "transpose_and_stack", "canonical_solution": ["\n    transposed_array = array.T\n    stacked_df = pd.concat([df, pd.DataFrame(transposed_array, columns=df.columns)])\n    return stacked_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'A': [1, 2, 3],\n        'B': [4, 5, 6]\n    })\n    array = np.array([[7, 8, 9], [10, 11, 12]])\n\n    result = candidate(array, sample_data)\n\n    expected_result = pd.DataFrame({\n        'A': [1, 2, 3, 7, 8, 9],\n        'B': [4, 5, 6, 10, 11, 12]\n    }, index=[0, 1, 2, 0, 1, 2])\n\n    assert result.equals(expected_result)\n\n    # Test Case 2: Empty DataFrame\n    empty_data = pd.DataFrame(columns=['A', 'B'])\n    array = np.array([[1, 2], [3, 4]])\n\n    result_empty = candidate(array, empty_data)\n\n    expected_result_empty = pd.DataFrame({\n        'A': [1, 2],\n        'B': [3, 4]\n    }, index=[0, 1], dtype='object')\n\n    assert result_empty.equals(expected_result_empty)"}
{"task_id": "PanNumEval/12", "prompt": "import pandas as pd\nimport numpy as np\n\ndef calculate_mean_and_median_sales_by_year(df):\n    ''' Given a DataFrame df with columns 'Year' and 'Sales', calculate the mean and median sales for each year using np.sum() and return a new DataFrame with the year, mean sales, and median sales. '''\n", "entry_point": "calculate_mean_and_median_sales_by_year", "canonical_solution": ["\n    year_sales_stats = df.groupby('Year')['Sales'].agg([np.mean, np.median]).reset_index()\n    year_sales_stats.columns = ['Year', 'Mean_Sales', 'Median_Sales']\n    return year_sales_stats"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'Year': [2020, 2020, 2021, 2021, 2022, 2022],\n        'Sales': [100, 150, 80, 120, 200, 300]\n    })\n\n    result_data = candidate(sample_data)\n    expected_result = pd.DataFrame({\n        'Year': [2020, 2021, 2022],\n        'Mean_Sales': [125, 100, 250],\n        'Median_Sales': [125, 100, 250]\n    })\n    assert result_data.equals(expected_result)\n\n    # Test Case 2: Different sales for the same year\n    different_sales_data = pd.DataFrame({\n        'Year': [2020, 2020, 2020, 2021, 2021],\n        'Sales': [100, 150, 200, 80, 120]\n    })\n\n    result_different_sales = candidate(different_sales_data)\n    expected_different_sales_result = pd.DataFrame({\n        'Year': [2020, 2021],\n        'Mean_Sales': [150, 100],\n        'Median_Sales': [150, 100]\n    })\n    assert result_different_sales.equals(expected_different_sales_result)\n\n"}
{"task_id": "PanNumEval/13", "prompt": "import pandas as pd\nimport numpy as np\n\n\ndef eigenvalue_and_column_selection(matrix, df):\n    ''' Given a square matrix and a DataFrame, calculate the eigenvalues of the matrix using np.linalg.eig(). Then,\n    select the columns from the DataFrame corresponding to the top k eigenvalues(Absolute value), where k is the number of\n    eigenvalues. Return the selected columns. '''\n", "entry_point": "eigenvalue_and_column_selection", "canonical_solution": ["\n    eigenvalues, _ = np.linalg.eig(matrix)\n    num_eigenvalues = len(eigenvalues)\n\n    if num_eigenvalues == 0:\n        return pd.DataFrame()\n    top_eigenvalue_indices = np.argsort(np.abs(eigenvalues))[-num_eigenvalues:]\n    selected_columns = df.iloc[:, top_eigenvalue_indices]\n    return selected_columns"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    matrix = np.array([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9]])\n    data = {'A': [4, 5, 6, 7],\n            'B': [8, 9, 10, 11],\n            'C': [12, 13, 14, 15]}\n    df = pd.DataFrame(data, dtype='int32')\n\n    result_df = candidate(matrix, df)\n    expected_result = df.iloc[:, [2, 1, 0]]\n    assert result_df.equals(expected_result)\n\n    # Test case 2: Matrix with all eigenvalues being zero\n    matrix = np.array([[0, 0],\n                       [0, 0]])\n    data = {'A': [4, 5],\n            'B': [8, 9]}\n    df = pd.DataFrame(data, dtype='int32')\n\n    result_df = candidate(matrix, df)\n    assert result_df.equals(df)\n\n    # Test case 3: Matrix with non-zero eigenvalues\n    matrix = np.array([[3, 2],\n                       [1, 4]])\n    data = {'A': [4, 5],\n            'B': [8, 9],\n            'C': [1, 2]}\n    df = pd.DataFrame(data, dtype='int32')\n\n    result_df = candidate(matrix, df)\n    expected_result = pd.DataFrame(data={'A': [4, 5], 'B': [8, 9]}, dtype='int32')\n    assert result_df.equals(expected_result)"}
{"task_id": "PanNumEval/14", "prompt": "import pandas as pd\nimport numpy as np\n\ndef reshape_and_slice(df):\n    '''\n    Given a DataFrame df with shape (6, 2), reshape it into a new DataFrame with shape (3, 4) using np.reshape(). Return the first 2 rows of the reshaped DataFrame.\n    '''", "entry_point": "reshape_and_slice", "canonical_solution": ["\n    reshaped_array = np.reshape(df.values, (3, 4))\n    reshaped_df = pd.DataFrame(reshaped_array, columns=['A', 'B', 'C', 'D'])\n    first_two_rows = reshaped_df.head(2)\n    return first_two_rows"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Test Case 1\n    data = {'A': [1, 2, 3, 4, 5, 6], 'B': [7, 8, 9, 10, 11, 12]}\n    df = pd.DataFrame(data)\n    result_df = candidate(df)\n    expected_values = [[1, 7, 2, 8], [3, 9, 4, 10]]\n    assert (result_df.values == expected_values).all()\n\n    # Test Case 2\n    data = {'A': [4, 5, 6, 1, 2, 3], 'B': [10, 11, 12, 7, 8, 9]}\n    df = pd.DataFrame(data)\n    result_df = candidate(df)\n\n    expected_values2 = [[4, 10, 5, 11], [6, 12, 1, 7]]\n    assert (result_df.values == expected_values2).all()\n"}
{"task_id": "PanNumEval/15", "prompt": "import pandas as pd\nimport numpy as np\n\ndef combine_and_sort(array1, array2, df, sort_column):\n    '''\n    Combine two numpy arrays with df.columns, sort the resulting DataFrame based on a given column,\n    and return the sorted DataFrame.\n    '''", "entry_point": "combine_and_sort", "canonical_solution": ["\n    concatenated_array = np.concatenate((array1, array2), axis=0)\n    concatenated_df = pd.DataFrame(concatenated_array, columns=df.columns)\n    sorted_df = concatenated_df.sort_values(by=sort_column)\n    return sorted_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    array1 = np.array([[1, 2, 3],\n                       [4, 5, 6]])\n    array2 = np.array([[10, 11, 12],\n                       [7, 8, 9]])\n    data = {'A': [4, 5],\n            'B': [8, 9],\n            'C': [12, 13]}\n    df = pd.DataFrame(data, dtype='int32')\n    sort_column = 'A'\n\n    result_df = candidate(array1, array2, df, sort_column)\n    expected_result = {'A': [1, 4, 7, 10],\n                       'B': [2, 5, 8, 11],\n                       'C': [3, 6, 9, 12]}\n    expected_df = pd.DataFrame(expected_result, dtype='int32')\n    assert (result_df.values == expected_df.values).all()\n\n    # Test Case 2: Edge case with empty DataFrame\n    empty_data = pd.DataFrame(columns=['A', 'B', 'C'], dtype='int32')\n    result_empty = candidate(array1, array2, empty_data, sort_column)\n    assert (result_empty.values == expected_df.values).all()\n\n    # Test Case 3: Sorting on a different column\n    data2 = {'A': [1, 2],\n             'B': [4, 5],\n             'C': [7, 8]}\n    df2 = pd.DataFrame(data2, dtype='int32')\n    sort_column2 = 'C'\n    result_df2 = candidate(array1, array2, df2, sort_column2)\n    expected_result2 = {'A': [1, 2, 4, 5],\n                        'B': [4, 5, 8, 7],\n                        'C': [7, 8, 3, 6]}\n    assert (result_df2.values == expected_df.values).all()"}
{"task_id": "PanNumEval/16", "prompt": "import pandas as pd\nimport numpy as np\n\ndef copy_and_sort(array1, array2, column_name, df):\n    ''' Given two numpy arrays, a column name, and a pandas DataFrame, copy the values from array1 and array2 into the specified column of the DataFrame and then sort the DataFrame based on that column. '''\n", "entry_point": "copy_and_sort", "canonical_solution": ["\n    np.copyto(df[column_name].values, np.concatenate((array1, array2)))\n    sorted_df = df.sort_values(by=column_name)\n\n    return sorted_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\n\ndef check(candidate):\n    # Test Case 1\n    array1 = np.array([3, 1, 4])\n    array2 = np.array([2, 5, 6])\n    column_name = 'Values'\n    data = {'Values': [7, 8, 9, 1, 2, 3]}\n    df = pd.DataFrame(data)\n\n    result_df = candidate(array1, array2, column_name, df)\n    expected_result = pd.DataFrame({'Values': [3, 1, 4, 2, 5, 6]})\n    expected_result = expected_result.sort_values(by='Values')\n\n    assert result_df.equals(expected_result)\n\n    # Test Case 2\n    array1 = np.array([1, 2, 3])\n    array2 = np.array([4, 5, 6])\n    column_name = 'Values'\n    data = {'Values': [7, 8, 9, 1, 2, 3]}\n    df = pd.DataFrame(data)\n\n    result_df = candidate(array1, array2, column_name, df)\n    expected_result = pd.DataFrame({'Values': [1, 2, 3, 4, 5, 6]})\n    expected_result = expected_result.sort_values(by='Values')\n\n    assert result_df.equals(expected_result)\n\n    # Test Case 3\n    array1 = np.array([10, 20])\n    array2 = np.array([5, 15])\n    column_name = 'Values'\n    data = {'Values': [7, 8, 9, 1]}\n    df = pd.DataFrame(data)\n\n    result_df = candidate(array1, array2, column_name, df)\n    expected_result = pd.DataFrame({'Values': [5, 10, 15, 20]})\n    expected_result = expected_result.sort_values(by='Values')\n\n    assert (result_df.values == expected_result.values).all()"}
{"task_id": "PanNumEval/17", "prompt": "import pandas as pd\nimport numpy as np\n\ndef process_and_concatenate(data_list):\n    ''' Given a list of dictionaries, where each dictionary represents a set of data, create a single DataFrame by concatenating the data. Then, split the DataFrame into two DataFrames using numpy.array_split(), where each DataFrame contains an equal number of rows. Return the two DataFrames. '''\n", "entry_point": "process_and_concatenate", "canonical_solution": ["\n    concatenated_df = pd.concat([pd.DataFrame(data) for data in data_list], ignore_index=True)\n    df_list = np.array_split(concatenated_df, 2)\n    return df_list[0], df_list[1]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    data_list = [\n        {'A': [1, 2, 3], 'B': [4, 5, 6]},\n        {'A': [7, 8, 9], 'B': [10, 11, 12]}\n    ]\n\n    result_df1, result_df2 = candidate(data_list)\n    expected_df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    expected_df2 = pd.DataFrame({'A': [7, 8, 9], 'B': [10, 11, 12]})  # Empty DataFrame since split is equal\n\n    assert (result_df1.values == expected_df1.values).all()\n    assert (result_df2.values == expected_df2.values).all()\n\n    # Test case 2: Unequal split\n    data_list_unequal = [\n        {'A': [1, 2, 3], 'B': [4, 5, 6]},\n        {'A': [7, 8], 'B': [10, 11]},\n        {'A': [7, 8], 'B': [10, 11]},\n    ]\n\n    result_unequal_df1, result_unequal_df2 = candidate(data_list_unequal)\n    expected_unequal_df1 = pd.DataFrame({'A': [1, 2, 3, 7], 'B': [4, 5, 6, 10]})\n    expected_unequal_df2 = pd.DataFrame({'A': [8, 7, 8], 'B': [11, 10, 11]})\n\n    assert (result_unequal_df1.values == expected_unequal_df1.values).all()\n    assert (result_unequal_df2.values == expected_unequal_df2.values).all()"}
{"task_id": "PanNumEval/18", "prompt": "import pandas as pd\nimport numpy as np\n\n\ndef update_array_and_locate(df, array, value_to_replace, new_value):\n    '''\n    Given a DataFrame df with columns 'ID' and 'Value', update the corresponding values in the provided array,\n    replace 'value_to_replace' with 'new_value', and return the indices of rows where the updated value matches the\n    'Value' column.\n    '''", "entry_point": "update_array_and_locate", "canonical_solution": ["\n    updated_array = np.where(array == value_to_replace, new_value, array)\n    df['Value'] = updated_array\n\n    matching_indices = df.loc[df['Value'] == new_value, 'ID'].tolist()\n\n    return matching_indices"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1\n    data = {'ID': [1, 2, 3, 4, 5],\n            'Value': [10, 20, 30, 40, 50]}\n    df = pd.DataFrame(data)\n    array = np.array([10, 20, 30, 40, 50])\n    value_to_replace = 30\n    new_value = 25\n    result_indices = candidate(df.copy(), array.copy(), value_to_replace, new_value)\n    expected_indices = [3]\n    assert result_indices == expected_indices\n\n    # Test Case 2\n    data = {'ID': [1, 2, 3, 4, 5],\n            'Value': [10, 20, 30, 40, 50]}\n    df = pd.DataFrame(data)\n    array = np.array([10, 20, 30, 40, 50])\n    value_to_replace = 20\n    new_value = 25\n    result_indices = candidate(df.copy(), array.copy(), value_to_replace, new_value)\n    expected_indices = [2]\n    assert result_indices == expected_indices\n\n    # Test Case 3: No matches\n    data = {'ID': [1, 2, 3, 4, 5],\n            'Value': [10, 20, 30, 40, 50]}\n    df = pd.DataFrame(data)\n    array = np.array([10, 20, 30, 40, 50])\n    value_to_replace = 60\n    new_value = 25\n    result_indices = candidate(df.copy(), array.copy(), value_to_replace, new_value)\n    expected_indices = []\n    assert result_indices == expected_indices"}
{"task_id": "PanNumEval/19", "prompt": "import pandas as pd\nimport numpy as np\n\ndef roll_and_filter_dataframe(df, shift_amount, threshold_value):\n    '''\n    Given a DataFrame df and two parameters, shift_amount and threshold_value, roll the columns of the DataFrame using numpy.roll() with the given shift_amount.\n    Then, filter out the rows where any column value is less than the threshold_value using df.loc().\n    Return the modified DataFrame.\n    '''", "entry_point": "roll_and_filter_dataframe", "canonical_solution": ["\n    rolled_df = pd.DataFrame(np.roll(df.values, shift_amount, axis=1), columns=df.columns)\n    filtered_df = rolled_df.loc[(rolled_df >= threshold_value).all(axis=1)]\n    return filtered_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    data = {\n        'A': [1, 2, 3, 4, 5],\n        'B': [6, 7, 8, 9, 10],\n        'C': [11, 12, 13, 14, 15]\n    }\n    df = pd.DataFrame(data)\n\n    result = candidate(df, shift_amount=1, threshold_value=5)\n\n    expected_result = pd.DataFrame({\n        'A': [15],\n        'B': [5],\n        'C': [10]\n    })\n    assert (result.values == expected_result.values).all()\n\n    # Test case 2: Edge case with empty DataFrame\n    empty_df = pd.DataFrame(columns=['A', 'B', 'C'])\n    result_empty = candidate(empty_df, shift_amount=2, threshold_value=0)\n    expected_result_empty = pd.DataFrame(columns=['A', 'B', 'C'])\n    assert result_empty.equals(expected_result_empty)\n\n    # Test case 3: Filtering out all rows\n    data_all_below_threshold = {\n        'A': [1, 2, 3],\n        'B': [2, 3, 4],\n        'C': [3, 4, 5]\n    }\n    df_all_below_threshold = pd.DataFrame(data_all_below_threshold)\n    result_all_below_threshold = candidate(df_all_below_threshold, shift_amount=1, threshold_value=10)\n    assert result_all_below_threshold.empty"}
{"task_id": "PanNumEval/20", "prompt": "import pandas as pd\nimport numpy as np\n\ndef extract_data_between_years(df, start_year, end_year):\n    '''\n    Given a DataFrame with columns 'Year' and 'Temperature', return a new DataFrame containing rows where the 'Year' column\n    falls between the start_year and end_year (inclusive) and the 'Temperature' is greater than 20.\n    '''", "entry_point": "extract_data_between_years", "canonical_solution": ["\n    filtered_df = df.loc[(df['Year'].isin(np.arange(start_year, end_year + 1))) & (df['Temperature'] > 20)]\n\n    return filtered_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'Year': [2019, 2020, 2021, 2022, 2023],\n        'Temperature': [15, 25, 22, 18, 30]\n    })\n\n    filtered_data = candidate(sample_data, start_year=2020, end_year=2022)\n    expected_result_1 = pd.DataFrame({\n        'Year': [2020, 2021],\n        'Temperature': [25, 22]\n    }, index=[1, 2])\n    assert filtered_data.equals(expected_result_1)\n\n    # Test case 2: Edge case with no matching rows\n    empty_data = pd.DataFrame(columns=['Year', 'Temperature'])\n    filtered_empty_data = candidate(empty_data, start_year=2020, end_year=2022)\n    expected_result_2 = pd.DataFrame(columns=['Year', 'Temperature'])\n    assert filtered_empty_data.equals(expected_result_2)\n\n    # Test case 3: All rows match, but temperature is not greater than 20\n    sample_data_2 = pd.DataFrame({\n        'Year': [2020, 2021, 2022],\n        'Temperature': [18, 19, 17]\n    })\n    filtered_data_2 = candidate(sample_data_2, start_year=2020, end_year=2022)\n    assert filtered_data_2.empty\n\n    # Test case 4: Multiple matches\n    sample_data_3 = pd.DataFrame({\n        'Year': [2019, 2020, 2021, 2022, 2023],\n        'Temperature': [25, 22, 26, 21, 30]\n    })\n    filtered_data_3 = candidate(sample_data_3, start_year=2020, end_year=2022)\n    expected_result_3 = pd.DataFrame({\n        'Year': [2020, 2021, 2022],\n        'Temperature': [22, 26, 21]\n    })\n    assert (filtered_data_3.values == expected_result_3.values).all()"}
{"task_id": "PanNumEval/21", "prompt": "import pandas as pd\nimport numpy as np\n\ndef concatenate_and_remove_duplicates(array1, array2, df):\n    '''\n    You are given two numpy arrays and a pandas DataFrame. Your task is to write a function to concatenate the arrays\n    and create a new DataFrame with given DataFrame's columns while removing any duplicate rows.\n    '''", "entry_point": "concatenate_and_remove_duplicates", "canonical_solution": ["\n    concatenated_array = np.concatenate((array1, array2), axis=0)\n    concatenated_df = pd.DataFrame(concatenated_array, columns=df.columns)\n    deduplicated_df = concatenated_df.drop_duplicates()\n    return deduplicated_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    array1 = np.array([[1, 2, 3],\n                       [4, 5, 6]])\n    array2 = np.array([[4, 5, 6],\n                       [7, 8, 9]])\n    columns = ['A', 'B', 'C']\n    df = pd.DataFrame(columns=columns, dtype='int32')\n\n    result = candidate(array1, array2, df)\n    expected_result = pd.DataFrame(np.concatenate((array1, array2), axis=0, dtype='int32'), columns=columns)\n    expected_result = expected_result.drop_duplicates()\n    assert result.equals(expected_result)\n\n    # Test Case 2: Edge case with empty DataFrame\n    array3 = np.array([[1, 2, 3],\n                       [6, 7, 9]])\n    array4 = np.array([[11, 12, 13],\n                       [7, 8, 9]])\n    empty_data = pd.DataFrame(columns=columns)\n    result_empty = candidate(array3, array4, empty_data)\n    expected_empty = pd.DataFrame(np.concatenate((array3, array4), axis=0, dtype='int32'), columns=columns)\n    expected_empty = expected_empty.drop_duplicates()\n    assert result_empty.equals(expected_empty)\n\n    # Test Case 3: DataFrame with duplicate rows\n    data = {'A': [1, 2, 3, 1],\n            'B': [4, 5, 6, 4],\n            'C': [7, 8, 9, 7]}\n    df_with_duplicates = pd.DataFrame(data, dtype='int32')\n    result_duplicates = candidate(array1, array2, df_with_duplicates)\n    expected_result_duplicates = pd.DataFrame(np.concatenate((array1, array2), axis=0, dtype='int32'), columns=columns)\n    expected_result_duplicates = expected_result_duplicates.drop_duplicates()\n    assert result_duplicates.equals(expected_result_duplicates)"}
{"task_id": "PanNumEval/22", "prompt": "import pandas as pd\nimport numpy as np\n\ndef count_nonzero_columns(df):\n    ''' Given a DataFrame df, write a function to calculate the number of non-zero elements in each column and return a Series containing these counts. '''\n", "entry_point": "count_nonzero_columns", "canonical_solution": ["\n    nonzero_counts = df.apply(np.count_nonzero)\n    return nonzero_counts"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    data = {\n        'A': [1, 0, 3, 0, 5],\n        'B': [0, 2, 0, 4, 0],\n        'C': [0, 0, 0, 0, 0]\n    }\n    df = pd.DataFrame(data)\n    result = candidate(df)\n    expected_result = pd.Series([3, 2, 0], index=['A', 'B', 'C'])\n    assert result.equals(expected_result)\n\n    # Test case 2: All zero values\n    data = {\n        'A': [0, 0, 0],\n        'B': [0, 0, 0]\n    }\n    df = pd.DataFrame(data)\n    result = candidate(df)\n    expected_result = pd.Series([0, 0], index=['A', 'B'])\n    assert result.equals(expected_result)\n\n    # Test case 3: Mixed non-zero and zero values\n    data = {\n        'A': [1, 2, 0],\n        'B': [0, 0, 3],\n        'C': [4, 0, 5]\n    }\n    df = pd.DataFrame(data)\n    result = candidate(df)\n    expected_result = pd.Series([2, 1, 2], index=['A', 'B', 'C'])\n    assert result.equals(expected_result)"}
{"task_id": "PanNumEval/23", "prompt": "import pandas as pd\nimport numpy as np\n\ndef map_unique_integers(array, df):\n    '''\n    Given a numpy array and a DataFrame with columns 'Integer' and 'Label',\n    map the unique integers by ascending order in the array to their corresponding labels and return a list of labels.\n    '''", "entry_point": "map_unique_integers", "canonical_solution": ["\n    unique_integers = np.unique(array)\n    integer_label_mapping = df.set_index('Integer')['Label'].to_dict()\n    mapped_labels = [integer_label_mapping[i] for i in unique_integers]\n    return mapped_labels"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    unique_integers = np.array([2, 3, 1])\n    data = {'Integer': [1, 2, 3],\n            'Label': ['LabelA', 'LabelB', 'LabelC']}\n    df = pd.DataFrame(data)\n    result = candidate(unique_integers, df)\n    expected_result = ['LabelA', 'LabelB', 'LabelC']\n    assert result == expected_result\n\n    # Test Case 2: Edge case with empty DataFrame\n    empty_data = pd.DataFrame(columns=['Integer', 'Label'])\n    empty_array = np.array([])\n    result_empty = candidate(empty_array, empty_data)\n    expected_result_empty = []\n    assert result_empty == expected_result_empty\n\n    # Test Case 3: Repeating integers\n    repeated_integers = np.array([1, 2, 2])\n    data2 = {'Integer': [1, 2, 3],\n             'Label': ['LabelX', 'LabelY', 'LabelZ']}\n    df2 = pd.DataFrame(data2)\n    result_repeated = candidate(repeated_integers, df2)\n    expected_result_repeated = ['LabelX', 'LabelY']\n    assert result_repeated == expected_result_repeated"}
{"task_id": "PanNumEval/24", "prompt": "import pandas as pd\nimport numpy as np\n\ndef split_names_and_create_dataframe(names_array, df):\n    '''\n    Given a numpy array of names and a DataFrame with a column 'Full Name',\n    split the full names into first names and last names, and return a new DataFrame with separate columns ('First Name' and 'Last Name').\n    '''", "entry_point": "split_names_and_create_dataframe", "canonical_solution": ["\n    first_names, last_names = np.vectorize(lambda x: x.split()[0])(names_array), np.vectorize(lambda x: x.split()[-1])(names_array)\n    new_df = pd.DataFrame({'First Name': first_names, 'Last Name': last_names})\n    return new_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'Full Name': ['John Doe', 'Jane Smith', 'Alice Johnson']\n    })\n\n    names_array = np.array(['John Doe', 'Jane Smith', 'Alice Johnson'])\n    result = candidate(names_array, sample_data)\n\n    expected_result = pd.DataFrame({\n        'First Name': ['John', 'Jane', 'Alice'],\n        'Last Name': ['Doe', 'Smith', 'Johnson']\n    })\n    assert result.equals(expected_result)\n\n    # Test Case 2: Names with middle names\n    middle_name_data = pd.DataFrame({\n        'Full Name': ['John A. Doe', 'Jane M. Smith', 'Alice J. Johnson']\n    })\n\n    middle_names_array = np.array(['John A. Doe', 'Jane M. Smith', 'Alice J. Johnson'])\n    middle_result = candidate(middle_names_array, middle_name_data)\n\n    expected_middle_result = pd.DataFrame({\n        'First Name': ['John', 'Jane', 'Alice'],\n        'Last Name': ['Doe', 'Smith', 'Johnson']\n    })\n    assert middle_result.equals(expected_middle_result)"}
{"task_id": "PanNumEval/25", "prompt": "import pandas as pd\nimport numpy as np\n\ndef process_dataframe(data, fill_value):\n    ''' You are given a pandas DataFrame 'data' with missing values. Your task is to fill the missing values with the provided 'fill_value', then calculate the cumulative sum of each column using np.cumsum(), and finally remove any duplicate rows using DataFrame.drop_duplicates(). Return the processed DataFrame. '''\n", "entry_point": "process_dataframe", "canonical_solution": ["\n    filled_data = data.fillna(fill_value)\n    cumulative_sum_data = filled_data.cumsum()\n    deduplicated_data = cumulative_sum_data.drop_duplicates()\n    return deduplicated_data"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'A': [1, np.nan, 3, 4],\n        'B': [np.nan, 5, np.nan, 7]\n    })\n    fill_value = 1\n    result = candidate(sample_data, fill_value)\n    expected_result = pd.DataFrame({\n        'A': [1, 2, 5, 9],\n        'B': [1, 6, 7, 14]\n    }, dtype='float64')\n    expected_result = expected_result.drop_duplicates()\n    assert result.equals(expected_result)\n\n    # Test Case 2: DataFrame with no missing values\n    data_no_missing = pd.DataFrame({\n        'A': [1, 2, 3],\n        'B': [4, 5, 6]\n    })\n    fill_value = 2\n    result_no_missing = candidate(data_no_missing, fill_value)\n    expected_result_no_missing = pd.DataFrame({\n        'A': [1, 3, 6],\n        'B': [4, 9, 15]\n    })\n    expected_result_no_missing = expected_result_no_missing.drop_duplicates()\n    assert result_no_missing.equals(expected_result_no_missing)\n\n    # Test Case 3: DataFrame with all missing values\n    all_missing_data = pd.DataFrame({\n        'A': [np.nan, np.nan, np.nan],\n        'B': [np.nan, np.nan, np.nan]\n    })\n    fill_value = 0\n    result_all_missing = candidate(all_missing_data, fill_value)\n    expected_result_all_missing = pd.DataFrame({\n        'A': [0, 0, 0],\n        'B': [0, 0, 0]\n    }, dtype='float64')\n    expected_result_all_missing = expected_result_all_missing.drop_duplicates()\n    assert result_all_missing.equals(expected_result_all_missing)"}
{"task_id": "PanNumEval/26", "prompt": "import pandas as pd\nimport numpy as np\n\ndef process_and_concatenate(data1, data2):\n    ''' You are given two pandas DataFrames 'data1' and 'data2', both having missing values. Your task is to fill the missing values in each DataFrame with 0 using df.fillna(), then calculate the cumulative sum of each DataFrame using np.cumsum(), and finally concatenate the two DataFrames while removing any duplicate rows using DataFrame.drop_duplicates(). Return the concatenated and processed DataFrame. '''\n",  "entry_point": "process_and_concatenate", "canonical_solution": ["\n    filled_data1 = data1.fillna(0)\n    filled_data2 = data2.fillna(0)\n    cumulative_sum_data1 = filled_data1.cumsum()\n    cumulative_sum_data2 = filled_data2.cumsum()\n    concatenated_data = pd.concat([cumulative_sum_data1, cumulative_sum_data2], axis=0)\n    deduplicated_data = concatenated_data.drop_duplicates()\n    return deduplicated_data"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    data1 = pd.DataFrame({\n        'A': [1, np.nan, 3],\n        'B': [4, np.nan, 6]\n    })\n    data2 = pd.DataFrame({\n        'A': [np.nan, 2, np.nan],\n        'B': [np.nan, 5, 7]\n    })\n\n    result = candidate(data1, data2)\n    filled_data1 = data1.fillna(0)\n    filled_data2 = data2.fillna(0)\n    cumulative_sum_data1 = filled_data1.cumsum()\n    cumulative_sum_data2 = filled_data2.cumsum()\n    expected_concatenated_data = pd.concat([cumulative_sum_data1, cumulative_sum_data2], axis=0)\n    expected_deduplicated_data = expected_concatenated_data.drop_duplicates()\n\n    assert result.equals(expected_deduplicated_data)\n\n    # Test case 2: Basic test with sample data\n    data1 = pd.DataFrame({\n        'A': [1, 3, 3],\n        'B': [4, 5, 6]\n    })\n    data2 = pd.DataFrame({\n        'A': [0, 2, 4],\n        'B': [9, 5, 7]\n    })\n\n    result = candidate(data1, data2)\n    filled_data1 = data1.fillna(0)\n    filled_data2 = data2.fillna(0)\n    cumulative_sum_data1 = filled_data1.cumsum()\n    cumulative_sum_data2 = filled_data2.cumsum()\n    expected_concatenated_data = pd.concat([cumulative_sum_data1, cumulative_sum_data2], axis=0)\n    expected_deduplicated_data = expected_concatenated_data.drop_duplicates()\n\n    assert result.equals(expected_deduplicated_data)\n\n    # Test case 3: Edge case with empty DataFrames\n    empty_data1 = pd.DataFrame(columns=['A', 'B'])\n    empty_data2 = pd.DataFrame(columns=['A', 'B'])\n\n    result_empty = candidate(empty_data1, empty_data2)\n    expected_empty = pd.DataFrame(columns=['A', 'B'])\n\n    assert result_empty.equals(expected_empty)"}
{"task_id": "PanNumEval/27", "prompt": "import pandas as pd\nimport numpy as np\n\ndef group_and_add_mean(data, category_col, value_col):\n    ''' Given a pandas DataFrame 'data' with columns 'category_col' and 'value_col', group the data by the category column and calculate the mean of the values using Pandas. Then, create a NumPy array containing the category names and their corresponding mean values, obtained by adding 10 to the calculated means. '''\n",  "entry_point": "group_and_add_mean", "canonical_solution": ["\n    grouped_data = data.groupby(category_col).mean()\n    category_names = grouped_data.index.values\n    mean_values = np.add(grouped_data[value_col], 10)\n    result_array = np.array([category_names, mean_values])\n    return result_array"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'Category': ['A', 'A', 'B', 'B', 'B', 'C'],\n        'Value': [5, 10, 15, 20, 25, 30]\n    })\n\n    result = candidate(sample_data, 'Category', 'Value')\n    expected_categories = np.array(['A', 'B', 'C'], dtype='str')\n    expected_means = np.array([17.5, 30, 40], dtype='float')\n\n    assert np.array_equal(result[0], expected_categories)\n    assert np.array_equal(result[1], expected_means)\n\n    # Test Case 2: Single category\n    single_category_data = pd.DataFrame({\n        'Category': ['A', 'A', 'A'],\n        'Value': [5, 10, 15]\n    })\n\n    result_single_category = candidate(single_category_data, 'Category', 'Value')\n\n    assert np.array_equal(result_single_category[0], ['A'])\n    assert np.array_equal(result_single_category[1], [20])"}
{"task_id": "PanNumEval/28", "prompt": "import pandas as pd\nimport numpy as np\n\ndef query_and_std(data, column_name, start_value, end_value):\n    ''' Given a pandas DataFrame 'data' with a column 'column_name', query the rows where the values in 'column_name' fall within the range of start_value and end_value. Calculate the standard deviation of the selected column values using NumPy's std(). Return the shape of the resulting data frame. '''\n", "entry_point": "query_and_std", "canonical_solution": ["\n    queried_data = data.query(f'{start_value} <= {column_name} <= {end_value}')\n    column_std = np.std(queried_data[column_name])\n    result_shape = queried_data.shape\n    return column_std, result_shape"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'Value': [10, 15, 20, 25, 30, 35, 40, 45, 50]\n    })\n\n    std1, result_shape_1 = candidate(sample_data, column_name='Value', start_value=20, end_value=40)\n\n    expected_shape_1 = (5, 1)  # Expecting 4 rows and 1 column\n    assert result_shape_1 == expected_shape_1\n    assert std1 - 7.07 < 0.01 and std1 -7.07 > -0.01\n\n    # Test Case 2: Edge case with no matching rows\n    std2, result_shape_2 = candidate(sample_data, column_name='Value', start_value=60, end_value=80)\n    expected_shape_2 = (0, 1)  # Expecting 0 rows and 1 column\n    assert result_shape_2 == expected_shape_2\n    assert np.isnan(std2)\n\n    # Test Case 3: Single row matching\n    std3, result_shape_3 = candidate(sample_data, column_name='Value', start_value=10, end_value=10)\n    expected_shape_3 = (1, 1)  # Expecting 1 row and 1 column\n    assert result_shape_3 == expected_shape_3\n    assert std3 - 0 < 0.01 and std3 - 0 > -0.01\n\n    # Test Case 4: Multiple columns in DataFrame\n    sample_data_2 = pd.DataFrame({\n        'Value': [10, 15, 20, 25, 30, 35, 40, 45, 50],\n        'Category': ['A', 'A', 'B', 'B', 'A', 'B', 'A', 'A', 'B']\n    })\n\n    std4, result_shape_4 = candidate(sample_data_2, column_name='Value', start_value=20, end_value=40)\n    expected_shape_4 = (5, 2)  # Expecting 4 rows and 2 columns\n    assert result_shape_4 == expected_shape_4\n    assert std4 - 7.07 < 0.01 and std4 - 7.07 > -0.01"}
{"task_id": "PanNumEval/29", "prompt": "import pandas as pd\nimport numpy as np\n\ndef filter_and_multiply(data, column_name, start_index, end_index, multiplier):\n    ''' Given a pandas DataFrame 'data' and a column 'column_name', filter the rows using loc[] to include only rows with indices between start_index and end_index (inclusive). Multiply the values in the selected column by the 'multiplier' using NumPy's multiply(). Finally, remove any duplicate rows and return the modified DataFrame. '''\n", "entry_point": "filter_and_multiply", "canonical_solution": ["\n    filtered_data = data.loc[start_index:end_index]\n    filtered_data[column_name] = np.multiply(filtered_data[column_name], multiplier)\n    modified_data = filtered_data.drop_duplicates()\n    return modified_data"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'Index': [0, 1, 2, 3, 4, 5],\n        'Value': [10, 20, 30, 40, 50, 60]\n    })\n\n    modified_data = candidate(sample_data, column_name='Value', start_index=1, end_index=4, multiplier=2)\n    expected_result_1 = pd.DataFrame({\n        'Index': [1, 2, 3, 4],\n        'Value': [40, 60, 80, 100]\n    }, index=[1, 2, 3, 4])\n    assert modified_data.equals(expected_result_1)\n\n    # Test Case 3: Multiplier is 1, no modification should occur\n    sample_data_2 = pd.DataFrame({\n        'Index': [0, 1, 2],\n        'Value': [100, 200, 300]\n    })\n    modified_data_2 = candidate(sample_data_2, column_name='Value', start_index=0, end_index=2, multiplier=1)\n    expected_result_3 = sample_data_2.copy()\n    assert modified_data_2.equals(expected_result_3)"}
{"task_id": "PanNumEval/30", "prompt": "import numpy as np\nimport pandas as pd\n\ndef calculate_cumulative_sum_and_groupby(arr):\n    '''Given a NumPy array , calculate the cumulative sum of the unique values and create a pandas DataFrame that groups the original array's values by their occurrences and sums them up.'''\n", "entry_point": "calculate_cumulative_sum_and_groupby", "canonical_solution": ["\n    unique_values = np.unique(arr)\n    cumulative_sum = np.cumsum(unique_values)\n    df = pd.DataFrame(arr, columns=['Value'])\n    grouped = df.groupby('Value').size().reset_index(name='Occurrences')\n    grouped_sum = grouped.groupby('Value')['Occurrences'].sum()\n    return cumulative_sum, grouped_sum"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test\n    input_array = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4])\n    cumulative_sum_result, grouped_sum_result = candidate(input_array)\n\n    unique_values = np.unique(input_array)\n    expected_cumulative_sum = np.cumsum(unique_values)\n\n    expected_grouped = pd.DataFrame({\n        'Value': unique_values,\n        'Occurrences': [1, 2, 3, 4]\n    })\n    expected_grouped_sum = expected_grouped.groupby('Value')['Occurrences'].sum()\n\n    assert np.array_equal(cumulative_sum_result, expected_cumulative_sum)\n    assert grouped_sum_result.equals(expected_grouped_sum)\n\n    # Test Case 2: Empty input array\n    empty_array = np.array([])\n    empty_cumulative_sum_result, empty_grouped_sum_result = candidate(empty_array)\n\n    assert empty_cumulative_sum_result.size == 0\n    assert empty_grouped_sum_result.empty\n\n    # Test Case 3: Input array with single value\n    single_value_array = np.array([5])\n    single_cumulative_sum_result, single_grouped_sum_result = candidate(single_value_array)\n\n    assert np.array_equal(single_cumulative_sum_result, single_value_array)\n    assert single_grouped_sum_result.index[0] == 5\n    assert single_grouped_sum_result.iloc[0] == 1"}
{"task_id": "PanNumEval/31", "prompt": "import numpy as np\nimport pandas as pd\n\ndef reshape_std_query_histogram():\n    '''Create a NumPy array of shape (20,) with values from 1 to 20. Reshape this array into a (4, 5) array. Calculate the standard deviation of the reshaped array and create a pandas DataFrame from it. Use the query() function to select rows where the values in the first column are greater than 2. Finally, compute the histogram of the second column.'''\n", "entry_point": "reshape_std_query_histogram", "canonical_solution": ["\n    original_array = np.arange(1, 21)\n    reshaped_array = np.reshape(original_array, (4, 5))\n    std_value = np.std(reshaped_array)\n    df = pd.DataFrame(reshaped_array, columns=['Col1', 'Col2', 'Col3', 'Col4', 'Col5'])\n    filtered_df = df.query('Col1 > 2')\n    hist, bin_edges = np.histogram(filtered_df['Col2'], bins=5)\n    return std_value, filtered_df, hist"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    std_value, filtered_df, hist = candidate()\n\n    # Check std_value\n    expected_std_value = np.std(np.reshape(np.arange(1, 21), (4, 5)))\n    assert np.isclose(std_value, expected_std_value, rtol=1e-5, atol=1e-8)\n\n    # Check filtered_df\n    expected_filtered_df = pd.DataFrame(np.reshape(np.arange(6, 21), (3, 5)),\n                                        columns=['Col1', 'Col2', 'Col3', 'Col4', 'Col5'])\n\n    assert (filtered_df.values == expected_filtered_df.values).all()\n\n    # Check hist\n    expected_hist, _ = np.histogram([7, 12, 17], bins=5)\n    np.testing.assert_array_equal(hist, expected_hist)"}
{"task_id": "PanNumEval/32", "prompt": "import pandas as pd\nimport numpy as np\n\n\ndef stack_drop_duplicates_pivot_sum():\n    '''Create two pandas DataFrames, each with two columns 'Name' and 'Value'. The first DataFrame has 5 rows with repeated 'Name' values ('A', 'B', 'C', 'A', 'B') and corresponding 'Value' values (1, 2, 3, 4, 5). The second DataFrame has 5 rows with repeated 'Name' values ('C', 'D', 'D', 'E', 'E') and corresponding 'Value' values (6, 7, 8, 9, 10). Vertically stack these two DataFrames using np.row_stack(). Drop any duplicate rows based on 'Name'. Then, create a pivot table with 'Name' as the index, 'Value' as the values, and the sum of values for each name.'''\n", "entry_point": "stack_drop_duplicates_pivot_sum", "canonical_solution": ["\n    df1 = pd.DataFrame({'Name': ['A', 'B', 'C', 'A', 'B'],\n                        'Value': [1, 2, 3, 4, 5]})\n\n    df2 = pd.DataFrame({'Name': ['C', 'D', 'D', 'E', 'E'],\n                        'Value': [6, 7, 8, 9, 10]})\n\n    stacked_df = pd.DataFrame(np.row_stack([df1.values, df2.values]), columns=['Name', 'Value'])\n    deduplicated_df = stacked_df.drop_duplicates(subset=['Name'])\n    pivot_table = deduplicated_df.pivot_table(index='Name', values='Value', aggfunc='sum')\n    return pivot_table"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Execute the function to be tested\n    result = candidate()\n\n    # Define expected results\n    expected_data = {'Value': [1, 2, 3, 7, 9]}\n    expected_result = pd.DataFrame(expected_data, index=['A', 'B', 'C', 'D', 'E'], columns=['Value'])\n\n    # Perform assertions\n    assert result.equals(expected_result)"}
{"task_id": "PanNumEval/33", "prompt": "import numpy as np\nimport pandas as pd\n\ndef region_sales_summary(sales_data):\n    '''You are given a Pandas DataFrame sales_data with columns 'Region', 'Product', and 'Sales'. Write a function that groups the DataFrame by 'Region', calculates the total sales, and then finds the maximum and minimum sales among all regions.'''\n", "entry_point": "region_sales_summary", "canonical_solution": ["\n    grouped_data = sales_data.groupby('Region')['Sales'].sum()\n    max_sales = np.max(grouped_data)\n    min_sales = np.min(grouped_data)\n    return max_sales, min_sales"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'Region': ['North', 'North', 'South', 'South', 'West', 'West'],\n        'Sales': [100, 150, 80, 200, 300, 50]\n    })\n\n    max_sales, min_sales = candidate(sample_data)\n    expected_max_sales = 350\n    expected_min_sales = 250  # Sum of sales in South region\n    assert max_sales == expected_max_sales\n    assert min_sales == expected_min_sales\n\n    # Test case 2: Edge case with empty DataFrame\n    empty_data = pd.DataFrame(columns=['Region', 'Sales'])\n    max_sales_empty, min_sales_empty = candidate(empty_data)\n    assert np.isnan(max_sales_empty)\n    assert np.isnan(min_sales_empty)\n\n    # Test case 3: Equal sales in all regions\n    equal_sales_data = pd.DataFrame({\n        'Region': ['A', 'B', 'C'],\n        'Sales': [100, 100, 100]\n    })\n    max_sales_equal, min_sales_equal = candidate(equal_sales_data)\n    expected_max_sales_equal = 100\n    expected_min_sales_equal = 100\n    assert max_sales_equal == expected_max_sales_equal\n    assert min_sales_equal == expected_min_sales_equal"}
{"task_id": "PanNumEval/34", "prompt": "import numpy as np\nimport pandas as pd\n\ndef filter_sqrt_values():\n    '''Generate a NumPy array of integer values ranging from 1 to 99 with 50 points. Convert this array into a Pandas DataFrame with a single column named 'Value'. Write a function that queries the DataFrame to find the rows where the square root of 'Value' is greater than 5. Return the shape of the resulting filtered DataFrame.'''\n", "entry_point": "filter_sqrt_values", "canonical_solution": ["\n    values = np.linspace(1, 99, 50, dtype='int32')\n    df = pd.DataFrame({'Value': values})\n    filtered_df = df.query('sqrt(Value) > 5')\n    filtered_shape = filtered_df.shape\n    return filtered_shape"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Generate the expected result\n    values = np.linspace(1, 99, 50, dtype='int32')\n    expected_df = pd.DataFrame({'Value': values})\n    expected_filtered_df = expected_df.query('sqrt(Value) > 5')\n    expected_shape = expected_filtered_df.shape\n\n    # Call the candidate function\n    result = candidate()\n\n    # Compare the result with the expected result\n    assert result == expected_shape"}
{"task_id": "PanNumEval/35", "prompt": "import numpy as np\nimport pandas as pd\n\ndef top_math_scores(scores):\n    '''You have a Pandas DataFrame scores with columns 'Student', 'Subject', and 'Score'. Write a function that filters the DataFrame to include only rows where the 'Score' is greater than or equal to 80 and the 'Subject' is 'Math'. After filtering, sort the DataFrame in descending order based on 'Score' and return the highest score.'''\n", "entry_point": "top_math_scores", "canonical_solution": ["\n    filtered_scores = scores.loc[(scores['Score'] >= 80) & (scores['Subject'] == 'Math')]\n    sorted_scores = filtered_scores.sort_values(by='Score', ascending=False)\n    highest_score = np.max(sorted_scores['Score'])\n    return highest_score"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'Student': ['Alice', 'Bob', 'Charlie', 'David'],\n        'Subject': ['Math', 'Math', 'Physics', 'Math'],\n        'Score': [85, 75, 90, 95]\n    })\n\n    result = candidate(sample_data)\n    expected_result = 95\n    assert result == expected_result\n\n    # Test Case 2: No valid scores\n    no_valid_scores_data = pd.DataFrame({\n        'Student': ['Eve', 'Frank'],\n        'Subject': ['Physics', 'Physics'],\n        'Score': [70, 65]\n    })\n\n    result_no_scores = candidate(no_valid_scores_data)\n    expected_result_no_scores = np.nan  # No valid score\n    assert np.isnan(result_no_scores) and np.isnan(expected_result_no_scores)\n\n    # Test Case 3: Multiple valid scores with ties\n    tied_scores_data = pd.DataFrame({\n        'Student': ['Grace', 'Helen', 'Ivy', 'Jack'],\n        'Subject': ['Math', 'Math', 'Math', 'Math'],\n        'Score': [85, 90, 90, 80]\n    })\n\n    result_tied_scores = candidate(tied_scores_data)\n    expected_result_tied_scores = 90  # There are two students with a score of 90\n    assert result_tied_scores == expected_result_tied_scores"}
{"task_id": "PanNumEval/27", "prompt": "import numpy as np\nimport pandas as pd\n\ndef find_high_scores(student_scores):\n    '''You have a Pandas DataFrame student_scores with columns 'Student', 'Subject', and 'Score'. Write a function that sorts the DataFrame based on 'Score' in ascending order. Then, find first row index where the 'Score' is equal to or greater than 85 using NumPy's np.searchsorted(). Finally, return the row corresponding to this index.'''\n", "entry_point": "find_high_scores", "canonical_solution": ["\n    sorted_scores = student_scores.sort_values(by='Score')\n    index_to_find = np.searchsorted(sorted_scores['Score'], 85, side='right')\n    row_with_high_score = sorted_scores.loc[index_to_find]\n    return row_with_high_score"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1\n    data = {'Name': ['Alice', 'Bob', 'Charlie'],\n            'Score': [75, 90, 85]}\n    df = pd.DataFrame(data)\n\n    result = candidate(df)\n\n    expected_result = pd.DataFrame({'Name': ['Charlie'], 'Score': [85]})\n    assert (result.values == expected_result.values).all()\n\n    # Test Case 2\n    data2 = {'Name': ['David', 'Emily', 'Frank'],\n             'Score': [70, 80, 95]}\n    df2 = pd.DataFrame(data2)\n\n    result2 = candidate(df2)\n\n    expected_result2 = pd.DataFrame({'Name': ['Frank'], 'Score': [95]})\n    assert (result2.values == expected_result2.values).all()"}
{"task_id": "PanNumEval/37", "prompt": "import numpy as np\nimport pandas as pd\n\ndef transpose_and_sort(data):\n    '''You have a Pandas DataFrame data with numerical values. Write a function that transposes the DataFrame and calculates the mean of non-null values in each row. Sort the rows based on the mean values in descending order '''\n", "entry_point": "transpose_and_sort", "canonical_solution": ["\n    transposed_data = np.transpose(data)\n    row_means = transposed_data.apply(pd.Series.dropna, axis=1).mean()\n    columns = data.columns\n    sorted_rows = transposed_data.loc[[columns[index] for index in row_means.sort_values(ascending=False).index]]\n    return sorted_rows"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'A': [1, 2, 3],\n        'B': [4, np.nan, 6],\n        'C': [7, 8, 9]\n    })\n\n    result_data = candidate(sample_data)\n    expected_result = pd.DataFrame({\n        'C': [7, 8, 9],\n        'B': [4, np.nan, 6],\n        'A': [1, 2, 3]\n    }).transpose()\n    assert result_data.equals(expected_result)\n\n    # Test Case 2: Empty DataFrame\n    empty_data = pd.DataFrame(columns=['A', 'B', 'C'])\n    result_empty_data = candidate(empty_data)\n\n    assert result_empty_data.empty\n\n    # Test Case 3: DataFrame without NaN values\n    data_no_nan = pd.DataFrame({\n        'A': [1, 2, 3],\n        'B': [4, 5, 6],\n        'C': [7, 8, 9]\n    })\n    result_no_nan = candidate(data_no_nan)\n    expected_no_nan = pd.DataFrame({\n        'C': [7, 8, 9],\n        'B': [4, 5, 6],\n        'A': [1, 2, 3]\n    }).transpose()\n\n    assert result_no_nan.equals(expected_no_nan)"}
{"task_id": "PanNumEval/38", "prompt": "import numpy as np\nimport pandas as pd\n\ndef log_sales_summary(sales_data):\n    '''You have a Pandas DataFrame sales_data with columns 'Region', 'Product', and 'Sales'. Write a function that filters the DataFrame to include only rows where the 'Region' is 'North' and the 'Sales' is greater than 100. Then, calculate the logarithm of 'Sales' for each 'Product'. Provide a summary of the logarithmic sales using the describe() function for each product after grouping by 'Product'.'''\n", "entry_point": "log_sales_summary", "canonical_solution": ["\n    filtered_data = sales_data.loc[(sales_data['Region'] == 'North') & (sales_data['Sales'] > 100)]\n    filtered_data['LogSales'] = np.log(filtered_data['Sales'])\n    product_summary = filtered_data.groupby('Product')['LogSales'].describe()\n    return product_summary"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'Product': ['A', 'B', 'A', 'B', 'A', 'C'],\n        'Region': ['North', 'North', 'North', 'South', 'North', 'North'],\n        'Sales': [150, 120, 200, 90, 180, 300]\n    })\n\n    result = candidate(sample_data)\n    filtered_data = sample_data.loc[(sample_data['Region'] == 'North') & (sample_data['Sales'] > 100)]\n    filtered_data['LogSales'] = np.log(filtered_data['Sales'])\n    product_summary = filtered_data.groupby('Product')['LogSales'].describe()\n\n    assert result.equals(product_summary)"}
{"task_id": "PanNumEval/39", "prompt": "import pandas as pd\nimport numpy as np\n\ndef reshape_and_convert_to_df():\n    '''Generate a NumPy array with 20 equally spaced integer values between 0 and 100. Reshape the array into a 4x5 matrix and convert it into a pandas DataFrame with columns 'A', 'B', 'C', 'D', and 'E'. Return the last 3 rows of the DataFrame.'''\n", "entry_point": "reshape_and_convert_to_df", "canonical_solution": ["\n    numpy_array = np.linspace(0, 100, 20, dtype='int32').reshape(4, 5)\n    df = pd.DataFrame(numpy_array, columns=['A', 'B', 'C', 'D', 'E'])\n    return df.tail(3)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Generate the expected DataFrame\n    numpy_array = np.linspace(0, 100, 20, dtype='int32').reshape(4, 5)\n    df = pd.DataFrame(numpy_array, columns=['A', 'B', 'C', 'D', 'E'])\n    expected_df = df.tail(3)\n\n    # Call the candidate function\n    result_df = candidate()\n\n    # Compare the candidate's result with the expected result\n    assert result_df.equals(expected_df)"}
{"task_id": "PanNumEval/40", "prompt": "import pandas as pd\nimport numpy as np\n\ndef data_process(df):\n    '''given a DataFrame with 'age' and 'height' columns, and then 'calculate the histogram of 'age values by bins = 3. Pivot the DataFrame to calculate the mean height for each age group (bin) and filter rows where the mean height is greater than one standard deviation above the mean. Return the pivoted table.'''\n", "entry_point": "data_process", "canonical_solution": ["\n    hist, bins = np.histogram(df['age'], bins=3)\n    df['age_group'] = pd.cut(df['age'], bins=bins, labels=['Group 1', 'Group 2', 'Group 3'])\n\n    pivot_table = df.pivot_table(values='height', index='age_group', aggfunc='mean')\n    mean_height = df['height'].mean()\n    std_dev_height = df['height'].std()\n    filtered_pivot_table = pivot_table.query('height > @mean_height + @std_dev_height')\n    return filtered_pivot_table\n"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'age': [25, 35, 40, 50, 22, 28, 45, 55],\n        'height': [160, 175, 180, 165, 155, 170, 175, 185]\n    })\n\n    result_data = candidate(sample_data)\n    assert result_data.empty\n\n    # Test case 2: Edge case with no filtered data\n    no_filter_data = pd.DataFrame({\n        'age': [25, 35, 40, 50],\n        'height': [160, 175, 180, 165]\n    })\n\n    result_no_filter_data = candidate(no_filter_data)\n    assert result_no_filter_data.empty\n\n    # Test case 3: All ages in one group\n    data = {'age': [25, 30, 28, 40, 35, 22, 50, 27, 32, 45],\n            'height': [160, 170, 165, 175, 180, 155, 165, 160, 175, 170]}\n    df = pd.DataFrame(data)\n    result_data = candidate(df)\n\n    assert (abs(result_data.values - [[176.66666667]]) < [[0.01]]).all()"}
{"task_id": "PanNumEval/41", "prompt": "import pandas as pd\nimport numpy as np\n\n\ndef sales_data_process(df):\n    ''' Given a DataFrame with 'category', 'sales', and 'expenses' columns, calculate the log-transformed average profit (sales - expenses) for each category, then sort the categories in descending order based on the average profit and return the sorted_categories.'''\n", "entry_point": "sales_data_process", "canonical_solution": ["\n    df['profit'] = df['sales'] - df['expenses']\n    profit_stats = df.groupby('category').agg({'profit': ['mean', 'count']})\n    profit_stats['log_avg_profit'] = np.log(profit_stats[('profit', 'mean')])\n\n    sorted_categories = profit_stats.sort_values(by=('log_avg_profit'), ascending=False)\n    return sorted_categories"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test case 1: Basic test with sample data\n    sample_data = pd.DataFrame({\n        'category': ['A', 'B', 'A', 'C', 'B', 'C'],\n        'sales': [100, 150, 80, 200, 300, 50],\n        'expenses': [50, 75, 40, 120, 200, 40]\n    })\n\n    result = candidate(sample_data)\n    expected_result = pd.DataFrame({\n        ('profit', 'mean'): [ 87.5, 45, 45],\n        ('profit', 'count'): [2, 2, 2],\n        'log_avg_profit': [4.471639, 3.806662, 3.806662]\n    }, index=['B', 'A', 'C'])\n    assert (abs(result.values - expected_result.values) < 0.01).all()\n\n\n    # Test case 2: Test with a single category\n    single_category_data = pd.DataFrame({\n        'category': ['A'] * 5,\n        'sales': [100, 120, 80, 150, 200],\n        'expenses': [50, 70, 40, 100, 150]\n    })\n    result_single_category = candidate(single_category_data)\n    expected_result_single = pd.DataFrame({\n        ('profit', 'mean'): [48],\n        ('profit', 'count'): [5],\n        'log_avg_profit': [3.87120101],\n    }, index=['A'])\n    assert (abs(result_single_category.values - expected_result_single.values) < 0.01).all()"}
{"task_id": "PanNumEval/42", "prompt": "import pandas as pd\nimport numpy as np\n\n\ndef dataFrame_process():\n    '''Create a NumPy array containing numbers from 1 to 20. Reshape this array into a 4x5 matrix. Convert the matrix into a Pandas DataFrame. Using the DataFrame, calculate the sum of each row and the product of each column. return the shape of the DataFrame, the last 3 rows, the sum of each row and the product of each column.'''\n", "entry_point": "dataFrame_process", "canonical_solution": ["\n    # Create NumPy array and reshape\n    num_array = np.arange(1, 21)\n    reshaped_array = np.reshape(num_array, (4, 5))\n\n    # Convert to DataFrame\n    df = pd.DataFrame(reshaped_array, columns=['C1', 'C2', 'C3', 'C4', 'C5'])\n\n    # Calculate row sum and column product using DataFrame apply\n    row_sum = df.apply(np.sum, axis=1)\n    col_product = df.apply(np.prod, axis=0)\n\n    return df.shape, df.tail(3), row_sum, col_product"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Call the candidate function to get the results\n    df_shape, last_3_rows, row_sum, col_product = candidate()\n\n    # Test the shape of the DataFrame\n    assert df_shape == (4, 5)\n\n    # Test the last 3 rows of the DataFrame\n    expected_last_3_rows = pd.DataFrame([[6, 7, 8, 9, 10],\n                                         [11, 12, 13, 14, 15],\n                                         [16, 17, 18, 19, 20]], columns=['C1', 'C2', 'C3', 'C4', 'C5'])\n    assert (last_3_rows.values == expected_last_3_rows.values).all()\n\n    # Test row sum\n    expected_row_sum = pd.Series([15, 40, 65, 90], name=0)\n    assert (row_sum.values == expected_row_sum.values).all()\n\n    # Test column product\n    expected_col_product = pd.Series([ 1056,  2856,  5616,  9576, 15000], index=['C1', 'C2', 'C3', 'C4', 'C5'])\n    assert (col_product.values == expected_col_product.values).all()"}
{"task_id": "PanNumEval/43", "prompt": "import pandas as pd\nimport numpy as np\n\n\ndef matrix_dot():\n    '''Create two NumPy arrays 'a' and 'b' containing consecutive numbers from 1 to 12. Reshape 'a' into a 3x4 matrix and 'b' into a 4x3 matrix. Convert both matrices into Pandas DataFrames with appropriate column names. Calculate the dot product of 'a' and 'b' matrices and print the result and return the mean of each column in the resulting DataFrame.'''\n", "entry_point": "matrix_dot", "canonical_solution": ["\n    # Create NumPy arrays and reshape\n    a = np.arange(1, 13)\n    reshaped_a = np.reshape(a, (3, 4))\n    reshaped_b = np.reshape(a, (4, 3))\n\n    # Convert to DataFrames\n    df_a = pd.DataFrame(reshaped_a, columns=['A1', 'A2', 'A3', 'A4'])\n    df_b = pd.DataFrame(reshaped_b, columns=['B1', 'B2', 'B3'])\n\n    # Calculate dot product of transposed matrices\n    dot_product = np.dot(df_a, df_b)\n\n    # Calculate mean of each column\n    column_means = dot_product.mean()\n\n    return column_means"  ], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    mean = candidate()\n\n    assert mean == 184"}
{"task_id": "PanNumEval/44", "prompt": "import numpy as np\nimport pandas as pd\n\ndef find_solution(start_num, end_num):\n    '''Generate an array of 9 consecutive numbers from start_num to end_num using NumPy's arange function. Reshape the array into a 3x3 matrix. Convert this matrix into a Pandas DataFrame. Sort the DataFrame by its index in descending order. Solve the equation Ax = B, where A is the sorted DataFrame and B is [10, 20, 30]. Return the solution.'''\n", "entry_point": "find_solution", "canonical_solution": ["\n    # Generate arange array and reshape\n    num_array = np.arange(start_num, end_num)\n    reshaped_array = np.reshape(num_array, (3, 3))\n\n    # Convert to DataFrame\n    df = pd.DataFrame(reshaped_array, index=['C', 'B', 'A'], columns=['X', 'Y', 'Z'])\n\n    # Sort DataFrame by index in descending order\n    sorted_df = df.sort_index(ascending=False)\n\n    # Coefficients matrix A and constants vector B\n    A = sorted_df.values\n    B = np.array([10, 20, 30])\n\n    # Solve the equation Ax = B\n    solution = np.linalg.solve(A, B)\n    return solution"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test the function with specific start_num and end_num\n    start_num = 4\n    end_num = 13\n    solution = candidate(start_num, end_num)\n\n    # Define expected solution for given start_num and end_num\n    expected_solution = np.array([2.97916667, 4.04166667, -3.6875])\n\n    # Check if the computed solution is close enough to the expected solution\n    assert np.allclose(solution, expected_solution)"}
{"task_id": "PanNumEval/45", "prompt": "import numpy as np\nimport pandas as pd\ndef melt_data_frame():\n    '''Generate a NumPy array of 50 evenly spaced values between 0 and 100. Create a histogram of these values with 10 bins. Convert the histogram data into a Pandas DataFrame. Use the describe() method to get summary statistics. Drop any rows with missing values. Melt the DataFrame to convert it from wide to long format and return the result.'''\n", "entry_point": "melt_data_frame", "canonical_solution": ["\n    values = np.linspace(0, 100, 50)\n    hist, bins = np.histogram(values, bins=10)\n\n    hist_data = {'Bin': bins[:-1], 'Frequency': hist}\n    hist_df = pd.DataFrame(hist_data)\n\n    summary_stats = hist_df.describe()\n    cleaned_df = hist_df.dropna()\n\n    melted_df = cleaned_df.melt(id_vars='Bin', value_vars='Frequency')\n    return melted_df"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Generate a NumPy array of 50 evenly spaced values between 0 and 100\n    values = np.linspace(0, 100, 50)\n\n    # Create a histogram of the values with 10 bins\n    hist, bins = np.histogram(values, bins=10)\n\n    # Convert histogram data into a Pandas DataFrame\n    hist_data = {'Bin': bins[:-1], 'Frequency': hist}\n    expected_hist_df = pd.DataFrame(hist_data)\n\n    # Calculate expected summary statistics\n    expected_summary_stats = expected_hist_df.describe()\n\n    # Drop any rows with missing values from expected DataFrame\n    expected_cleaned_df = expected_hist_df.dropna()\n\n    # Melt the expected cleaned DataFrame to long format\n    expected_melted_df = expected_cleaned_df.melt(id_vars='Bin', value_vars='Frequency')\n\n    # Get the result from the candidate function\n    result_df = candidate()\n\n    # Check if the result matches the expected melted DataFrame\n    assert result_df.equals(expected_melted_df)"}
{"task_id": "PanNumEval/46", "prompt": "import numpy as np\nimport pandas as pd\n\n\ndef array_process():\n    '''Generate a NumPy array of 100 values between 0 and 100. Create a histogram of these values with 5 bins. Convert the histogram data into a Pandas DataFrame. Group the data by bin and calculate the sum and mean for each bin. Pivot the DataFrame to show bins as columns and the sum and mean as values. Drop any rows with missing values. Return the cleaned table'''\n", "entry_point": "array_process", "canonical_solution": ["\n    values = np.linspace(0, 100, 100)\n    hist, bins = np.histogram(values, bins=5)\n\n    hist_data = {'Bin': bins[:-1], 'Frequency': hist}\n    hist_df = pd.DataFrame(hist_data)\n\n    grouped = hist_df.groupby('Bin').agg({'Frequency': ['sum', 'mean']})\n    grouped.columns = ['Sum', 'Mean']\n\n    pivot_table = grouped.pivot_table(index='Bin', values=['Sum', 'Mean'])\n\n    cleaned_table = pivot_table.dropna()\n\n\n    return cleaned_table"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Generate the expected cleaned table manually\n    expected_result = pd.DataFrame({\n        ('Mean',): [20.0, 20.0, 20.0, 20.0, 20.0],\n        ('Sum',): [20.0, 20.0, 20.0, 20.0, 20.0]\n    }, index=[0, 1, 2, 3, 4])\n\n    # Call the candidate function\n    result = candidate()\n\n    # Check if the result matches the expected_result\n    assert np.allclose(expected_result, result)"}
{"task_id": "PanNumEval/47", "prompt": "import numpy as np\nimport pandas as pd\n\n\ndef manipulate_arrays(n , m):\n    '''Given two integer n, m. Create two 2D NumPy arrays, one filled with zeros n*m and the other with ones m*n. Transpose the second array and concatenate both arrays along rows. Convert the result into a DataFrame and return its last 5 rows.'''\n", "entry_point": "manipulate_arrays", "canonical_solution": ["\n    zeros_array = np.zeros((n, m))\n    ones_array = np.ones((m, n))\n\n    transposed_ones = np.transpose(ones_array)\n    combined_array = np.concatenate((zeros_array, transposed_ones), axis=0)\n\n    df = pd.DataFrame(combined_array)\n    last_rows = df.tail(5)\n\n    return last_rows"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1\n    n = 3\n    m = 4\n    result_df = candidate(n, m)\n    expected_result = pd.DataFrame([[0.0, 0.0, 0.0, 0.0],\n                                   [0.0, 0.0, 0.0, 0.0],\n                                    [1.0, 1.0, 1.0, 1.0],\n                                   [1.0, 1.0, 1.0, 1.0],\n                                   [1.0, 1.0, 1.0, 1.0],],\n                                   columns=[0, 1, 2, 3])\n    expected_result.index = [1, 2, 3, 4, 5]\n    assert np.allclose(result_df.values, expected_result.values)\n\n    # Test Case 2\n    n = 1\n    m = 1\n    result_df = candidate(n, m)\n    expected_result = pd.DataFrame([[0.0], [1.0]], columns=[0])\n    expected_result.index = [0, 1]\n    assert np.allclose(result_df.values, expected_result.values)\n\n    # Test Case 3\n    n = 0\n    m = 0\n    result_df = candidate(n, m)\n    expected_result = pd.DataFrame()\n    assert np.allclose(result_df.values, expected_result.values)"}
{"task_id": "PanNumEval/48", "prompt": "import numpy as np\nimport pandas as pd\n\n\ndef matrix_operations(start_num1, end_num1, start_num2, end_num2):\n    '''Create two 2x3 NumPy arrays using linspace with different ranges(start_num1, end_num1), (start_num2, end_num2). Perform element-wise addition and multiplication on these arrays, and then calculate the dot product. Convert the result into a DataFrame. Return addition_result, multiplication_result, df_shape'''\n",  "entry_point": "matrix_operations", "canonical_solution": ["\n    array1 = np.linspace(start_num1, end_num1, 6).reshape(2, 3)\n    array2 = np.linspace(start_num2, end_num2, 6).reshape(2, 3)\n\n    addition_result = np.add(array1, array2)\n    multiplication_result = np.multiply(array1, array2)\n    dot_product_result = np.dot(array1, array2.T)\n\n    df = pd.DataFrame(dot_product_result)\n    df_shape = df.shape\n\n    return addition_result, multiplication_result, df_shape"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Test Case 1\n    start_num1 = 1\n    end_num1 = 10\n    start_num2 = 11\n    end_num2 = 20\n    addition_result, multiplication_result, df_shape = candidate(start_num1, end_num1, start_num2, end_num2)\n\n    expected_addition_result = np.array([[12, 15.6, 19.2],\n                                         [22.8, 26.4, 30]])\n    expected_multiplication_result = np.array([[11, 35.84, 67.16],\n                                               [104.96, 149.24, 200]])\n    expected_df_shape = (2, 2)\n\n    assert np.allclose(addition_result, expected_addition_result)\n    assert np.allclose(multiplication_result, expected_multiplication_result)\n    assert df_shape == expected_df_shape\n\n    # Test Case 2\n    start_num1 = 0\n    end_num1 = 5\n    start_num2 = 5\n    end_num2 = 10\n    addition_result, multiplication_result, df_shape = candidate(start_num1, end_num1, start_num2, end_num2)\n\n    expected_addition_result = np.array([[5, 7, 9],\n                                         [11, 13, 15]])\n    expected_multiplication_result = np.array([[0, 6, 14],\n                                               [24, 36, 50]])\n    expected_df_shape = (2, 2)\n\n    assert np.allclose(addition_result, expected_addition_result)\n    assert np.allclose(multiplication_result, expected_multiplication_result)\n    assert df_shape == expected_df_shape"}
{"task_id": "PanNumEval/49", "prompt": "import numpy as np\nimport pandas as pd\n\ndef array_pandas():\n    '''Create a 3x3 NumPy array filled with zeros. Vertically stack this array on top of itself to create a 6x3 array. Convert the resulting array into a Pandas DataFrame with columns 'A', 'B', and 'C'. Sort the DataFrame by column 'B' in descending order. Select the first two rows. Group the DataFrame by column 'C' and count the number of occurrences in each group. Return selected rowss and the number counted before '''\n",  "entry_point": "array_pandas", "canonical_solution": ["\n    zero_array = np.zeros((3, 3))\n    stacked_array = np.vstack((zero_array, zero_array))\n    df = pd.DataFrame(stacked_array, columns=['A', 'B', 'C'])\n\n    df_sorted = df.sort_values(by='B', ascending=False)\n    selected_rows = df_sorted.iloc[:2]\n    grouped_counts = df_sorted.groupby('C').count()\n    return selected_rows, grouped_counts"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'array'\n}\n\n\ndef check(candidate):\n    # Execute the candidate function\n    selected_rows, grouped_counts = candidate()\n\n    # Create expected results\n    expected_zero_array = np.zeros((3, 3))\n    expected_stacked_array = np.vstack((expected_zero_array, expected_zero_array))\n    expected_df = pd.DataFrame(expected_stacked_array, columns=['A', 'B', 'C'])\n    expected_df_sorted = expected_df.sort_values(by='B', ascending=False)\n    expected_selected_rows = expected_df_sorted.iloc[:2]\n    expected_grouped_counts = expected_df_sorted.groupby('C').count()\n\n    # Compare the results\n    assert np.allclose(selected_rows.values, expected_selected_rows.values)\n    assert np.allclose(grouped_counts.values, expected_grouped_counts.values)"}